<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    .hp-photo{ width:240px; height:240px; border-radius:240px; -webkit-border-radius:240px; -moz-border-radius:240px; }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 15px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    </style>

    <title>SemanticHD | Tencent AI Lab </title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/Tencent.png">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
<tbody>
<tr>
<td>

    <!--SECTION 4 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td><heading> Our Research <!--<a href="https://vlar-group.github.io/paper.html"> (Full list at vLAR research page) </a>--> </heading>
          </td>
          </tr></tbody>
    </table>

    <!--SECTION 5 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody><tr>

	<!-- ACR -->
        <td width="20%"><img src="./imgs/22_neurips_ogc.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/2303.05938">
	             <papertitle>ACR: Attention Collaboration-based Regressor for Arbitrary Two-Hand Reconstruction</papertitle></a>
                     <br>
		     <a href="https://github.com/ZhengdiYu">Zhengdi Yu</a>,
		     <a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en">Shaoli Huang</a>,
		     <a href="http://fangchen.org/">Chen Fang</a>
		     <br>
                 <em>Conference on Computer Vision and Pattern Recognition (CVPR) </em>, 2023
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2303.05938">arXiv</a> /
                 <a href="https://semanticdh.github.io/ACR/"><font color="red">Project Page</font></a>/
                 <a href="https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=ZhengdiYu&repo=Arbitrary-Hands-3D-Reconstruction&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We introduce the first one-stage arbitrary hand reconstruction method using only a monocular RGB image as input.</p>
                <p></p>
            </td>
        </tr>


	<!-- BoPR -->
        <td width="20%"><img src="./imgs/22_neurips_unsupobjseg.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/2210.02324">
	             <papertitle>Promising or Elusive? Unsupervised Object Segmentation from Real-world Single Images</papertitle></a>
                 <br>Y. Yang, <strong>B. Yang</strong>
                 <br>
                 <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/2210.02324">arXiv</a> /
                 <a href="https://vlar-group.github.io/UnsupObjSeg.html"><font color="red">Project Page</font></a>/
                 <a href="https://github.com/vLAR-group/UnsupObjSeg"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=vLAR-group&repo=UnsupObjSeg&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We systematically investigate the effectiveness of existing unsupervised models on challenging real-world images.</p>
                <p></p>
            </td>
        </tr>


	<!-- HMC -->
        <td width="20%"><img src="./imgs/21_iccv_grf.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
                 <p><a href="http://arxiv.org/abs/2010.04595">
                 <papertitle>HMC: Hierarchical Mesh Coarsening for Skeleton-free Motion Retargeting</papertitle></a>
                 <br>A. Trevithick, <strong>B. Yang</strong> <br>
                 <em>IEEE International Conference on Computer Vision (ICCV)</em>, 2021
                 <br>
                 <a href="http://arxiv.org/abs/2010.04595">arXiv</a> /
                 <a href="https://github.com/alextrevithick/GRF"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=alextrevithick&repo=GRF&type=star&count=true&size=small" frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                 <p align="justify" style="font-size:13px">We introduce a simple implicit neural function to represent complex 3D geometries purely from 2D images.
                 </p>
                <p></p>
            </td>
        </tr>

        <td width="20%"><img src="./imgs/19_neurips_3d_bonet.gif" alt="PontTuset" width="180" style="border-style: none"></td>
            <td width="80%" valign="top">
	             <p><a href="https://arxiv.org/abs/1906.01140">
	             <papertitle>Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds</papertitle></a>
                 <br><strong>B. Yang</strong>, J. Wang, R. Clark, Q. Hu, S. Wang, A. Markham, N. Trigoni
                 <br>
                 <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2019 <font color="red"><strong>(Spotlight, 200/6743)</strong></font>
                 <br>
                 <!--<font color="red"><strong>..</strong></font><br>-->
                 <a href="https://arxiv.org/abs/1906.01140">arXiv</a> /
                 <a href="http://kaldir.vc.in.tum.de/scannet_benchmark/result_details?id=118">ScanNet Benchmark</a> /
                 <a href="https://www.reddit.com/r/MachineLearning/comments/bx8jhz/r_new_sota_for_3d_object_detection/">Reddit Discussion</a> /
                 <font color="red"> News:</font>
                 <a href="https://mp.weixin.qq.com/s/jHbWf_SSZE_J6NRJR-96sQ"><font color="red">(新智元,</font></a>
                 <a href="https://mp.weixin.qq.com/s/4GPkmTri4Vk7Xy0J8TiBNw"><font color="red">图像算法,</font></a>
                 <a href="https://mp.weixin.qq.com/s/C1FDPkAkmnmAZ_gvvtzBHw"><font color="red">AI科技评论,</font></a>
                 <a href="https://mp.weixin.qq.com/s/wViZITtsb4j3oFtOpJI9wQ"><font color="red">将门创投,</font></a>
                 <a href="https://mp.weixin.qq.com/s/S7mHrOxOwTIhDGPhu1SI4A"><font color="red">CVer,</font></a>
                 <a href="https://mp.weixin.qq.com/s/gybhVw3D4ykAHsVGzazWNw"><font color="red">泡泡机器人)</font>/</a>
                 <a href="https://www.youtube.com/watch?v=Bk727Ec10Ao"><font color="red">Video</font></a>/
                 <a href="https://github.com/Yang7879/3D-BoNet"><font color="red">Code</font></a>
                 <iframe src="https://ghbtns.com/github-btn.html?user=Yang7879&repo=3D-BoNet&type=star&count=true&size=small"
                 frameborder="0" scrolling="0" width="100px" height="20px"></iframe>
			     <p align="justify" style="font-size:13px">We propose a simple and efficient neural architecture for accurate 3D instance segmentation on point clouds.
                  It achieves the SOTA performance on ScanNet and S3DIS (June 2019).</p>
                <p></p>
            </td>
        </tr>

	
        </tbody>
    </table>

    <!--SECTION 7 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
       <tbody><tr>
          <td><heading>Members <a href=""> (Full list at ...)</a> </heading>
              <p> <strong><a href="https://scholar.google.com/citations?user=o31BPFsAAAAJ&hl=en&oi=ao">Shaoli Huang</a></strong>: &ensp;&ensp; Visual Computing Center, Tencent AI Lab.</p>
          </td>
       </tr></tbody>
    </table>

    <!--SECTION 9 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
    <td width="100%" align="middle">
    <p align="center" style="width: 33% ">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=FAcW1eE0mLMSgDewwZS9lllfyB0DKCYtyN0uTXn1ZHY"></script>
    </p></td>
    </tr>
    </tbody>
    </table>


    <!--SECTION 10 -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
         <tbody><tr>
            <td><br>
               <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
		       <p align="right"><font size="2"> Last update: 2023.03.20 <!--<a href="http://www.cs.berkeley.edu/~barron/">Thanks.</a></font></p>-->
            </td>
         </tr>
         </tbody>
     </table>


</td>
</tr>
</tbody>
</table>
</body>
</html>
